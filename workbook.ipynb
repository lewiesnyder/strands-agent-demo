{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strands agent with an MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession, stdio_client, StdioServerParameters\n",
    "from strands import Agent\n",
    "from strands.models.ollama import OllamaModel\n",
    "from strands.tools.mcp import MCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strans only supports tools, not prompts or resources\n",
    "\n",
    "Let's get the database schema from the MCP server the old fashioned way and add it to the system prompt.\n",
    "\n",
    "Uses [mcp-experiemnt](https://github.com/lewiesnyder/mcp-experiment) to build a simple sql agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get therethereschema from MCP\n",
    "stdio_params = StdioServerParameters(command=\"docker\", args=[\"run\", \"-i\", \"--rm\", \"mcp-experiment\"])\n",
    "async with stdio_client(stdio_params) as (read_stream, write_stream):\n",
    "    async with ClientSession(read_stream, write_stream) as session:\n",
    "        await session.initialize()\n",
    "        meta, schema = await session.read_resource(\"schema://main\")\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a helpful assistant that has access to various tools. Answer the user's queries to the best of your ability.\n",
    "\n",
    "You have access to the following schema for a database of music artists:\n",
    "{schema[1][0].text}\n",
    "\n",
    "## Requirements\n",
    "- THINK before you act.\n",
    "- OBSERVE after you act.\n",
    "- THINK again before you act again.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using llama3.2 hosted in ollama locally\n",
    "\n",
    "You could use the [litellm provider](https://strandsagents.com/latest/user-guide/concepts/model-providers/litellm/) and use the llm of your choice:\n",
    "```python\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from strands.models.litellm import LiteLLMModel\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ollama\n",
    "model = LiteLLMModel(\n",
    "    model_id=\"ollama/llama3.2\"\n",
    ")\n",
    "\n",
    "# claude\n",
    "model = LiteLLMModel(\n",
    "    client_args={\n",
    "        \"api_key\": os.get('ANTHROPIC_API_KEY'),\n",
    "    },\n",
    "    # **model_config\n",
    "    model_id=\"anthropic/claude-3-7-sonnet-20250219\",\n",
    "    params={\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7,\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OllamaModel(\n",
    "    host=\"http://localhost:11434\",\n",
    "    model_id=\"llama3.2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the MCP client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_client = MCPClient(lambda: stdio_client(stdio_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tool #1: query_data\n",
      "It seems that the SQL query is not correctly formatted. SQLite does not support the `TOP` keyword, which is typically used in Microsoft SQL Server.\n",
      "\n",
      "Here's a revised version of the query that uses the `ORDER BY` clause to retrieve the top 3 tracks by sales:\n",
      "\n",
      "\n",
      "Tool #2: query_data\n",
      "\n",
      "Tool #3: query_data\n",
      "The top three tracks by sales are:\n",
      "\n",
      "1. \"The Trooper\" with a total of 4.95\n",
      "2. \"Walkabout\" with a total of 3.98\n",
      "3. \"The Woman King\" with a total of 3.98"
     ]
    }
   ],
   "source": [
    "with mcp_client:\n",
    "    tools = mcp_client.list_tools_sync()\n",
    "    agent = Agent(system_prompt=system_prompt, model=model, tools=tools)\n",
    "\n",
    "    agent(\"What are the top three tracks by sales?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
